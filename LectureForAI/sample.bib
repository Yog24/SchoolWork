@ARTICLE{Vaswani:2017at,
 author = {Vaswani, A. and Shazeer, N. and Parmar, N. and Uszkoreit, J. and Jones, L. and Gomez, A.~N. and Kaiser, L. and Polosukhin, I.},
 title = {Attention is all you need},
 journal = {Advances in Neural Information Processing Systems},
 volume = {30},
 pages = {5998-6008},
 year = {2017}
 }

@ARTICLE{Radford:2018tf,
 author = {Radford, A. and Narasimhan, K. and Salimans, T. and Sutskever, I.},
 title = {Improving language understanding by generative pre-training},
 journal = {OpenAI Research},
 year = {2018}
 }

@ARTICLE{Devlin:2018br,
 author = {Devlin, J. and Chang, M.~W. and Lee, K. and Toutanova, K.},
 title = {BERT: Pre-training of deep bidirectional transformers for language understanding},
 journal = {Proceedings of the NAACL-HLT},
 volume = {1},
 pages = {4171-4186},
 year = {2018}
 }

@ARTICLE{Bender:2021ai,
 author = {Bender, E.~M. and Gebru, T. and McMillan-Major, A. and Shmitchell, S.},
 title = {On the dangers of stochastic parrots: Can language models be too big?},
 journal = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
 pages = {610-623},
 year = {2021}
 }

@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@misc{wu2024roleplay,
      title={From Role-Play to Drama-Interaction: An LLM Solution}, 
      author={Weiqi Wu and Hongqiu Wu and Lai Jiang and Xingyuan Liu and Jiale Hong and Hai Zhao and Min Zhang},
      year={2024},
      eprint={2405.14231},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.14231}, 
}

@misc{hinton2015distillingknowledgeneuralnetwork,
      title={Distilling the Knowledge in a Neural Network}, 
      author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
      year={2015},
      eprint={1503.02531},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1503.02531}, 
}

@misc{gao2024retrievalaugmentedgenerationlargelanguage,
      title={Retrieval-Augmented Generation for Large Language Models: A Survey}, 
      author={Yunfan Gao and Yun Xiong and Xinyu Gao and Kangxiang Jia and Jinliu Pan and Yuxi Bi and Yi Dai and Jiawei Sun and Meng Wang and Haofen Wang},
      year={2024},
      eprint={2312.10997},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.10997}, 
}

@misc{website,
  author = {Amazon Web Services, Inc.},
  title = { "What is RAG? - Retrieval-Augmented Generation AI Explained - AWS"},
  howpublished = {\url{https://aws.amazon.com/what-is/retrieval-augmented-generation/}},
  year = {2024},
  note = {16 July}
}

@misc{he2024doespromptformattingimpact,
      title={Does Prompt Formatting Have Any Impact on LLM Performance?}, 
      author={Jia He and Mukund Rungta and David Koleczek and Arshdeep Sekhon and Franklin X Wang and Sadid Hasan},
      year={2024},
      eprint={2411.10541},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.10541}, 
}

@misc{website-llm-market,
  author = {QYResearch, Inc.},
  title = {2024-2030 Global and China Large Language Model (LLM) Market Status and Forecast},
  howpublished = {\url{https://www.qyresearch.com.cn/reports/3752570/large-language-model--llm}},
  year = {2024},
  note = {17 May}
}

@misc{zhao2024surveylargelanguagemodels,
      title={A Survey of Large Language Models}, 
      author={Wayne Xin Zhao and Kun Zhou and Junyi Li and Tianyi Tang and Xiaolei Wang and Yupeng Hou and Yingqian Min and Beichen Zhang and Junjie Zhang and Zican Dong and Yifan Du and Chen Yang and Yushuo Chen and Zhipeng Chen and Jinhao Jiang and Ruiyang Ren and Yifan Li and Xinyu Tang and Zikang Liu and Peiyu Liu and Jian-Yun Nie and Ji-Rong Wen},
      year={2024},
      eprint={2303.18223},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.18223}, 
}

@misc{website-llm-mulins,
  author = {Prospective Industry Research Institute},
  title = {2024-2029 China Large Model Industry Development Prospects and Investment Strategy Planning Analysis Report},
  howpublished = {\url{https://www.qianzhan.com/analyst/detail/220/240716-295ffeeb.html}},
  year = {2024},
  note = {17 July}
}

@misc{csdn-person-rec,
  author = {User},
  title = {Paper Introduction | Application of Personalized Large Model in Recommendation Field},
  howpublished = {\url{https://blog.csdn.net/m0_59163425/article/details/143894373?fromshare=blogdetail&sharetype=blogdetail&sharerId=143894373&sharerefer=PC&sharesource=qq_39653587&sharefrom=from_link}},
  year = {2024},
  note = {19 Nov.}
}

@misc{stackoverflow-dataprivacy,
  author = {Sean Falconer},
  title = {Privacy in the age of generative AI},
  howpublished = {\url{https://stackoverflow.blog/2023/10/23/privacy-in-the-age-of-generative-ai/}},
  year = {2023},
  note = {23 Oct.}
}

@misc{yang2024problematictokenstokenizerbias,
      title={Problematic Tokens: Tokenizer Bias in Large Language Models}, 
      author={Jin Yang and Zhiqiang Wang and Yanbin Lin and Zunduo Zhao},
      year={2024},
      eprint={2406.11214},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.11214}, 
}


@inproceedings{10.1145/3442188.3445922,
author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
title = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ðŸ¦œ},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445922},
doi = {10.1145/3442188.3445922},
abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {610â€“623},
numpages = {14},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@misc{unnes,
  author = {UNNEW},
  title = {Challenging systematic prejudices: an investigation into bias against women and girls in large language models},
  howpublished = {\url{https://unesdoc.unesco.org/ark:/48223/pf0000388971}},
  year = {2024}
}

@misc{unres,
  author = {UNESCO and International Research Center on Artificial Intelligence},
  title = {Generative AI exacerbates gender bias},
  howpublished = {\url{https://news.un.org/zh/story/2024/03/1127197}},
  year = {2024}
}

@misc{gptcost,
  author = {The Heart of Machine},
  title = {GPT-4 model architecture, training cost, and data set information have all been revealed},
  howpublished = {\url{https://www.jiqizhixin.com/articles/2023-07-11-7}},
  year = {2023}
}

@misc{reducecost,
  author = {Gongji HashRate},
  title = {A company of Distributed Computing in AI},
  howpublished = {\url{https://www.gongjiyun.com/}},
  
}

@misc{googlereport,
  author = {Google, Inc.},
  title = {Google 2022 Environmental Report},
  howpublished = {\url{https://sustainability.google/reports/google-2022-environmental-report/}},
  year = {2022}
}

